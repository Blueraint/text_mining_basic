import time
import random
import pandas as pd
from datetime import datetime, timedelta

# Selenium 및 Undetected Chromedriver 관련 라이브러리 임포트
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

def get_post_details(driver, post_url):
    """
    개별 게시물 페이지에 접속하여 상세 정보를 크롤링하는 함수
    """
    driver.execute_script(f"window.open('{post_url}');")
    driver.switch_to.window(driver.window_handles[1])
    time.sleep(random.uniform(2, 4)) # 페이지 로딩 대기

    try:
        # WebDriverWait를 사용하여 주요 콘텐츠 영역이 로드될 때까지 대기
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.article-wrapper"))
        )

        # --- 데이터 추출 ---
        title = driver.find_element(By.CSS_SELECTOR, "h1.article-title").text
        author = driver.find_element(By.CSS_SELECTOR, "span.user-name").text
        
        # 날짜와 조회수 정보 추출 및 분리
        sub_info_text = driver.find_element(By.CSS_selector, "div.sub-info").text
        date_str = sub_info_text.split(' · ')[1]
        views = int(sub_info_text.split(' · ')[2].replace('조회 ', '').replace(',', ''))

        content = driver.find_element(By.CSS_SELECTOR, "div#content-view").text
        
        # 태그 추출 (없는 경우 대비)
        try:
            tags = [tag.text for tag in driver.find_elements(By.CSS_SELECTOR, "div.tag-list button")]
        except NoSuchElementException:
            tags = []

        # 댓글 추출 (없는 경우 대비)
        replies = []
        try:
            reply_elements = driver.find_elements(By.CSS_SELECTOR, "div.reply-item")
            for reply in reply_elements:
                try:
                    reply_author = reply.find_element(By.CSS_SELECTOR, "span.user-name").text
                    reply_content = reply.find_element(By.CSS_SELECTOR, "p").text
                    replies.append(f"{reply_author}: {reply_content}")
                except NoSuchElementException:
                    replies.append("내용을 찾을 수 없는 댓글") # 블라인드 처리된 댓글 등
        except NoSuchElementException:
            replies = []

        post_data = {
            "title": title,
            "author": author,
            "date": date_str,
            "views": views,
            "content": content,
            "tags": ", ".join(tags), # 리스트를 문자열로 변환
            "replies": "\n".join(replies) # 개행으로 댓글 구분
        }

    except TimeoutException:
        print(f"  - 페이지 로드 시간 초과: {post_url}")
        post_data = None
    except Exception as e:
        print(f"  - 데이터 추출 중 에러 발생: {e}")
        post_data = None
    finally:
        # 현재 탭 닫고 원래 탭으로 복귀
        driver.close()
        driver.switch_to.window(driver.window_handles[0])
        time.sleep(random.uniform(1, 2))

    return post_data

def crawl_blind(max_posts=None, target_date=None):
    """
    블라인드 부동산 토픽을 크롤링하는 메인 함수
    :param max_posts: 수집할 최대 게시물 수
    :param target_date: 'YYYY-MM-DD' 형식의 날짜, 이 날짜 이후의 게시물만 수집
    """
    if target_date:
        target_date_obj = datetime.strptime(target_date, "%Y-%m-%d").date()
    
    options = uc.ChromeOptions()
    # options.add_argument("--headless") # 창 없이 실행하려면 주석 해제
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    driver = uc.Chrome(options=options)

    base_url = "https://www.teamblind.com/kr/topics/부동산"
    driver.get(base_url)
    time.sleep(3)

    crawled_data = []
    post_links = set() # 중복 링크 방지를 위해 set 사용

    last_height = driver.execute_script("return document.body.scrollHeight")
    
    # 지정된 개수나 날짜에 도달할 때까지 스크롤하며 링크 수집
    while True:
        # 현재 화면의 게시물 링크 수집
        articles = driver.find_elements(By.CSS_SELECTOR, "a.article-list-item")
        for article in articles:
            post_links.add(article.get_attribute('href'))

        print(f"현재까지 수집된 링크 수: {len(post_links)}")

        # 크롤링 종료 조건 확인
        stop_crawling = False
        if max_posts and len(post_links) >= max_posts:
            print(f"\n목표 게시물 수({max_posts}개)에 도달하여 수집을 중단합니다.")
            stop_crawling = True
        
        if target_date:
            try:
                # 페이지의 마지막 게시물 날짜 확인
                last_post_date_str = articles[-1].find_element(By.CSS_SELECTOR, ".article-info-date").text
                if '전' in last_post_date_str or ':' in last_post_date_str: # '방금 전', '1분 전' 등
                    post_date = datetime.now().date()
                else:
                    post_date = datetime.strptime(last_post_date_str, "%Y.%m.%d").date()

                if post_date < target_date_obj:
                    print(f"\n목표 날짜({target_date}) 이전의 게시물에 도달하여 수집을 중단합니다.")
                    stop_crawling = True
            except (NoSuchElementException, IndexError):
                print("날짜 정보를 찾을 수 없어 스크롤을 계속합니다.")
            except ValueError:
                 # 날짜 형식이 다른 경우 (예: 어제) 처리
                 if '어제' in last_post_date_str:
                     post_date = datetime.now().date() - timedelta(days=1)
                     if post_date < target_date_obj:
                         stop_crawling = True
                 else:
                    print(f"날짜 형식 변환 오류: {last_post_date_str}")
        
        if stop_crawling:
            break

        # 페이지 맨 아래로 스크롤
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(2, 4)) # 새 게시물 로딩 대기

        # 새 높이와 이전 높이가 같으면 더 이상 로드할 게시물이 없는 것이므로 종료
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            print("\n페이지의 끝에 도달했습니다.")
            break
        last_height = new_height

    # 수집된 링크들을 기반으로 상세 정보 크롤링
    final_links = list(post_links)[:max_posts] if max_posts else list(post_links)
    print(f"\n총 {len(final_links)}개의 게시물에 대한 상세 정보 크롤링을 시작합니다.")

    for i, link in enumerate(final_links):
        print(f"({i+1}/{len(final_links)}) 크롤링 중: {link}")
        details = get_post_details(driver, link)
        if details:
            # 날짜 조건이 있는 경우, 상세 페이지에서 다시 한번 날짜 확인
            if target_date:
                detail_date_obj = datetime.strptime(details['date'], "%Y.%m.%d").date()
                if detail_date_obj >= target_date_obj:
                    crawled_data.append(details)
            else:
                crawled_data.append(details)
    
    driver.quit()

    # 데이터프레임 생성 및 CSV 파일로 저장
    if not crawled_data:
        print("\n크롤링된 데이터가 없습니다.")
        return

    df = pd.DataFrame(crawled_data)
    
    # 날짜를 기준으로 정렬
    df['date_dt'] = pd.to_datetime(df['date'], format='%Y.%m.%d')
    df = df.sort_values(by='date_dt', ascending=False).drop(columns=['date_dt'])

    filename = f"blind_realestate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"\n크롤링 완료! 총 {len(df)}개의 게시물이 '{filename}' 파일로 저장되었습니다.")

if __name__ == "__main__":
    # --- 실행 옵션 선택 ---
    
    # 옵션 1: 최근 50개 게시물 크롤링
    crawl_blind(max_posts=50)

    # 옵션 2: 특정 날짜(예: 2025년 6월 15일) 이후의 모든 게시물 크롤링
    # crawl_blind(target_date="2025-06-15")
    
    # 옵션 3: 최대 200개 내에서, 2025년 6월 10일 이후 게시물 크롤링
    # crawl_blind(max_posts=200, target_date="2025-06-10")