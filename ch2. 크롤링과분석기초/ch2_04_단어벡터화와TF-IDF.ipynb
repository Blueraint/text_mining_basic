{"cells":[{"cell_type":"markdown","metadata":{"id":"H_vZSCpe-gT4"},"source":["## **텍스트마이닝(윤상혁 교수)**\n","## ch2. 웹크롤링과 분석기초\n","## ch2_04. 단어벡터화와TF-IDF\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBIC6ap4XNO6"},"outputs":[],"source":["# 불피요한 warnings 이 길게 출력되는 막기 위한 코드이다.\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFlB8K5kIhC9"},"outputs":[],"source":["# 데이터 분석을 위한 pandas, 수치계산을 위한 numpy, 시각화를 위한 seaborn, matplotlib 을 불러온다.\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"6b5RxWcKNTEf"},"source":["한글폰트 설정을 위한 koreanize-matplotlib가 설치 되어 있지 않다면 아래 코드의 주석(#)을 지우고 셀을 실행하여 설치한다. colab에서는 노트북을 새로 열 때마다 설치해야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t66iEHVtLiZn"},"outputs":[],"source":["!pip install koreanize-matplotlib"]},{"cell_type":"markdown","metadata":{"id":"AyTPG7XiLiZo"},"source":["## 시각화를 위한 한글폰트 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29oJws6LgzIR"},"outputs":[],"source":["import koreanize_matplotlib\n","\n","# 그래프에 retina display 적용\n","%config InlineBackend.figure_format = 'retina'\n","\n","pd.Series([1, 3, 5, -7, 9]).plot(title=\"한글\", figsize=(6, 1))"]},{"cell_type":"markdown","metadata":{"id":"d74ZK7ypN9Xn"},"source":["##단어 가방 모형 만들기"]},{"cell_type":"markdown","metadata":{"id":"Rhy1ATcYrSgi"},"source":["### 분석의 순서     \n"," ●\t분석하고자 하는 데이터를 corpus에 담는다. 여기서는 임의로 4개의 문장을 담았다.         \n","●\t sklearn.feature_extraction.text 에서 CountVectorizer() 를 불러온다.     \n","●\tfit() 에 데이터(corpus)를 넣어 단어 사전을 학습시킨다.     \n","●\ttransform() 메서드를 통해 수치 행렬 형태로 변환한다.     \n"]},{"cell_type":"markdown","metadata":{"id":"5yjAbiffZaTj"},"source":["먼저 데이터를 ‘Corpus’에 담아 둔다. 여기서는 임의로 4개의 문장을 담았다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yv-6jXPabA2E"},"outputs":[],"source":["corpus = [\"코로나 거리두기와 코로나 상생지원금 문의입니다.\",\n","          \"지하철 운행시간과 지하철 요금 문의입니다.\",\n","          \"지하철 승강장 문의입니다.\",\n","          \"택시 승강장 문의입니다.\"]\n","\n","corpus"]},{"cell_type":"markdown","metadata":{"id":"jIJi8b8kZ7TP"},"source":["● sklearn.feature_extraction.text 에서 CountVectorizer() 를 통해 BOW를 생성한다.\n","\n","* API documentation:https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWfpkk8YtKUN"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"markdown","metadata":{"id":"l4srC0k-rV-I"},"source":["● 다음으로 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 Document Term Matrix(이하 dtm) 을 반환하고 fit() 에 데이터(corpus)를 넣어 단어 사전을 학습시킨다. fit()은  모든 토큰의 어휘 사전을 학습한다.           \n","● 마지막으로 transform(): 문서를 단어 빈도수가 들어있는 문서 용어 매트릭스로 변환한다."]},{"cell_type":"markdown","metadata":{"id":"pdXYRQqgQS2V"},"source":["### fit, transform, fit_transfrom의 차이점\n","* fit(): 원시 문서에 있는 모든 토큰의 어휘 사전을 배운다.\n","* transform(): 문서를 문서 용어 매트릭스로 변환합니다. transform 이후엔 매트릭스로 변환되어 숫자형태로 변경된다.\n","* fit_transform(): 어휘 사전을 배우고 문서 용어 매트릭스를 반환한다. fit 다음에 변환이 오는 것과 동일하지만 더 효율적으로 구현된다.\n","\n","⚠️ 주의! ⚠️\n","* 단, fit_transform 은 학습데이터에만 사용하고 예측 데이터에는 transform 을 사용한다.\n","* 예측 데이터에도 fit_transform 을 사용하게 된다면 서로 다른 단어사전으로 행렬을 만들게 된다.\n","* fit 과 transform 을 따로 사용해 준다 하더라도 fit 은 학습 데이터에만 사용한다. 같은 단어 사전으로 예측 데이터셋의 단어 사전을 만들기 위해서이다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kj92S-EBF-yB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"v9QNg-o4u0WM"},"source":["또 다른 방법으로는 fit_transform()을 사용하여 효율적으로 구현할 수도 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnrEbah0LiZs"},"outputs":[],"source":["dtm = cvect.fit_transform(corpus)\n","dtm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXSatDPzF-vt"},"outputs":[],"source":["# 단어사전을 확인해 보면  {\"단어\": 인덱스번호} 로 되어 있음을 알 수 있다.\n"]},{"cell_type":"markdown","metadata":{"id":"kGrFys2kxMk1"},"source":["get_feature_names_out()을 사용하면 dtm 이라는 변수로 쓰여진 단어-문서 행렬에 등장하는 순서대로 단어 사전을 반환한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jFX2ts0LiZs"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"JyiMVQk2xYkE"},"source":["이제 document-term matrix를 판다스의 데이터프레임으로 만들어서 단어의 빈도를 확인할 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXr_Vzj_LiZt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"k55XjvMxykhu"},"source":["전체 문서에는 등장하지만, 해당 문서에는 등장하지 않는 단어는 0으로 표시된다. 예시 문서의 빈도수를 보면 첫 번째 문서에서 “코로나\"라는 단어가 2번 등장하기 때문에 빈도수가 2로 표시가 되어 있다.  이제  이제 전체 문서에서 단어 빈도의 합계를 구하는 것으로 데이터를 간명하게 보이도록 요약한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06asmxUbLiZt"},"outputs":[],"source":["# T는 가로로 길게 보이기 위해 위해 추가한 것이다.\n","df_dtm.sum().to_frame().T"]},{"cell_type":"markdown","metadata":{"id":"GIPr3DaCPohL"},"source":["### N-grams\n","\n","* 토큰을 몇 개 사용할 것인지를 구분합니다. 지정한 n개의 숫자 만큼의 토큰을 묶어서 사용한다.\n","* 예를 들어 (1, 1) 이라면 1개의 토큰을 (2, 3)이라면 2~3개의 토큰을 사용한다.\n","* analyzer 설정에 따라 단어단위, 캐릭터 단위에 따라 사용할 수 있다.\n","\n","* 기본값 = (1, 1)\n","* ngram_range(min_n, max_n)\n","* min_n <= n <= max_n\n","```\n","(1, 1) 은 1 <= n <= 1\n","(1, 2) 은 1 <= n <= 2\n","(2, 2) 은 2 <= n <= 2\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmfShZP9LiZu"},"outputs":[],"source":["# 단어가 너무 많아서 출력이 오래 걸린다면 max_columns 값을 조정해서 사용한다.\n","# pd.options.display.max_columns = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15LxBmxkHg8U"},"outputs":[],"source":["# ngram_range: 추출할 다른 단어 n-gram 또는 char n-gram에 대한 n-값 범위의 하한 및 상한이다. 기본값 = (1, 1)\n","# ngram_range=(1, 2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-xp5TjsLiZu"},"outputs":[],"source":["#get_feature_names_out()을 사용하여 dtm 변수에 쓰여진 단어-문서 행렬에 등장하는 순서대로 단어 사전을 반환한다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqOPEF3i41qe"},"outputs":[],"source":["# df_dtm.sum 으로 빈도수 합계를 구한다.\n"]},{"cell_type":"markdown","metadata":{"id":"dWEivLuaJ5te"},"source":["### min_df\n","\n","* 기본값=1\n","* min_df는 문서 빈도(문서의 %에 있음)가 지정된 임계값보다 엄격하게 낮은 용어를 무시한다.\n","* 예를 들어, min_df=0.66은 용어가 어휘의 일부로 간주되려면 문서의 66%에 나타나야 한다.\n","* 때때로 min_df가 어휘 크기를 제한하는 데 사용된다.\n","* 예를들어 min_df를 0.1, 0.2로 설정한다면 10%, 20%에 나타나는 용어만 학습한다."]},{"cell_type":"markdown","metadata":{"id":"MdwIi9dOLoHb"},"source":["### max_df\n","\n","* 기본값=**1**\n","* max_df=int : 빈도수를 의미한다.\n","* max_df=float : 비율을 의미한다.\n","* 어휘를 작성할 때 주어진 임계값보다 문서 빈도가 엄격히 높은 용어는 무시한다.\n","* 빈번하게 등장하는 불용어 등을 제거하기에 편리하다.\n","* 예를 들어 코로나 관련 기사를 분석하면 90%에 '코로나'라는 용어가 등장할 수 있는데, 이 경우 max_df=0.89 로 비율을 설정하여 너무 빈번하게 등장하는 단어를 제외할 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScvG4AS9LiZv"},"outputs":[],"source":["cvect = CountVectorizer(ngram_range=(1, 3), min_df=0.2, max_df=5)\n","dtm = cvect.fit_transform(corpus)\n","vocab = cvect.get_feature_names_out()\n","df_dtm = pd.DataFrame(dtm.toarray(), columns=vocab)\n","df_dtm"]},{"cell_type":"markdown","metadata":{"id":"2IseYq_4RZk5"},"source":["### max_features\n","\n","* 기본값 = None\n","* Vectorizer가 학습할 기능(어휘)의 양 제한\n","* corpus중 빈도수가 가장 높은 순으로 해당 갯수만큼만 추출"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ad5WAo1PyVI"},"outputs":[],"source":["# max_features : 갯수만큼의 단어만 추출\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-IkahjKiLiZv"},"source":["### 불용어 stop_words\n","\n","* 문장에 자주 등장하지만 \"우리, 그, 그리고, 그래서\" 등 관사, 전치사, 조사, 접속사 등의 단어로 문장 내에서 큰 의미를 갖지 않는 단어"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-Na0Nw9LiZv"},"outputs":[],"source":["stop_words=[\"코로나\"]\n","# max_features 갯수만큼의 단어만 추출하기\n","# \"문의입니다\"도 불용어로 추가해보기\n","\n","cvect = CountVectorizer(ngram_range=(1, 3), min_df=1, max_df=1.0, max_features=20, stop_words=stop_words)\n","dtm = cvect.fit_transform(corpus)\n","vocab = cvect.get_feature_names_out()\n","df_dtm = pd.DataFrame(dtm.toarray(), columns=vocab)\n","df_dtm"]},{"cell_type":"markdown","metadata":{"id":"FWIgHaWFbYyZ"},"source":["### analyzer\n","\n","* 기본값='word'\n","* 종류: word, char, char_wb\n","* 기능을 단어 n-그램으로 만들지 문자 n-그램으로 만들어야 하는지 여부이다. 옵션 'char_wb'는 단어 경계 내부의 텍스트에서만 문자 n-gram을 생성한다. 단어 가장자리의 n-gram은 공백으로 채워진다.\n","* 띄어쓰기가 제대로 되어 있지 않은 문자 등에 사용할 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sW3jX8omddHS"},"outputs":[],"source":["# analyzer='char', ngram_range=(2, 3)\n","\n","cvect = CountVectorizer(analyzer='char',\n","                        ngram_range=(1, 5), min_df=2,\n","                        max_df=1.0, max_features=30,\n","                        stop_words=stop_words)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rJFrsTk0m-kR"},"source":["## 4.2. TF-IDF\n","\n","\\begin{equation*}\n","\\text{tfidf}(w, d) = \\text{tf} \\times (\\log\\big(\\frac{N + 1}{N_w + 1}\\big) + 1)\n","\\end{equation*}\n","\n","\n","### TfidfVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jyOWdiGTCSR"},"outputs":[],"source":["# sklearn.feature_extraction.text 에서 TfidfVectorizer 를 불러 온다.\n","# fit, transform 으로 변환한다.\n","# tfidfvect\n","from sklearn.feature_extraction.text import TfidfVectorizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ponMyHGLiZw"},"outputs":[],"source":["# fit_transform 으로 변환할 수도 있다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQDvbCt5LiZw","scrolled":false},"outputs":[],"source":["# dtm.toarray() 로 배열을 확인한다.\n","# 문서에 토큰이 더 많이 나타날수록 가중치는 더 커진다.\n","# 그러나 토큰이 문서에 많이 표시될수록 가중치가 감소한다.\n","dtm.toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"af0blkAyLiZx","scrolled":true},"outputs":[],"source":["# display_transform_dtm 으로 변환 결과를 확인한다.\n"]},{"cell_type":"markdown","metadata":{"id":"i2j5WlNa_der"},"source":["# **추가 정보**"]},{"cell_type":"markdown","metadata":{"id":"XNrqEJNYS1GX"},"source":["### IDF\n","\n","**IDF**\n","\n","- IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어 '원자'라는 낱말은 일반적인 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkaUmQ_rTCMj"},"outputs":[],"source":["# 하나의 문서에만 나타나는 토큰은 idf 가중치가 높다.\n","# 적게 나타난 토큰이라도 모든 문서에도 있는 토큰은 idf가 낮다.\n","idf = tfidfvect.idf_\n","idf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikcpPibOUiSz"},"outputs":[],"source":["# 사전만들기\n","# dict, zip 을 사용하여 피처명과 idf 값을 딕셔너리 형태로 만든다.\n","# idf_dict\n","vocab = tfidfvect.get_feature_names_out()\n","idf_dict = dict(zip(vocab, idf))\n","idf_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bGwhiCMLiZx"},"outputs":[],"source":["# idf_dict 값 시각화\n","pd.Series(idf_dict).plot.barh()"]},{"cell_type":"markdown","metadata":{"id":"hgy7OqhRLiZx"},"source":["### TfidfVectorizer 의 다양한 기능 사용하기\n","* analyzer\n","* n-gram\n","* min_df, max_df\n","* max_features\n","* stop_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxrR99jEk714"},"outputs":[],"source":["# analyzer='char_wb', ngram_range=(2, 3), max_df=1.0, min_df=1\n","tfidfvect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), max_df=1.0, min_df=1)\n","dtm = tfidfvect.fit_transform(corpus)\n","vocab = tfidfvect.get_feature_names_out()\n","df_dtm = pd.DataFrame(dtm.toarray(), columns=vocab)\n","print(\"단어 수 : \", len(vocab))\n","print(vocab)\n","display(df_dtm.style.background_gradient())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2K3F_dXRz5Wj"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}