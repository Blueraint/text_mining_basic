{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **텍스트마이닝(윤상혁 교수)**\n",
        "## ch3. 토픽모델링\n",
        "## ch3_01. LDA토픽모델링\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "U7SSzMdZ9q4H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In7uwLfAFXlH"
      },
      "outputs": [],
      "source": [
        "# konlpy 패키지 설치: 한국어 형태소 분석을 위한 라이브러리입니다.\n",
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXMa0XqhFYAS"
      },
      "outputs": [],
      "source": [
        "# pandas 라이브러리를 pd라는 이름으로 임포트: 데이터 처리를 위한 주요 라이브러리입니다.\n",
        "import pandas as pd\n",
        "\n",
        "# konlpy 라이브러리에서 Okt 형태소 분석기를 임포트: 한국어 텍스트를 형태소 단위로 분석합니다.\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 엑셀 파일을 데이터프레임 형태로 불러오기: 분석할 뉴스 데이터를 불러옵니다.\n",
        "df = pd.read_excel('/content/빅카인즈_주식_뉴스.xlsx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-G2THcI01qX"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ECUD_qG0yjy"
      },
      "outputs": [],
      "source": [
        "# '제목'과 '본문' 컬럼을 합쳐 '제목_본문'이라는 새로운 컬럼을 생성합니다.\n",
        "df['제목_본문'] = df['제목'] + df['본문']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIEcd5tYILYS"
      },
      "outputs": [],
      "source": [
        "df['제목_본문']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUkmicb-72Qr"
      },
      "outputs": [],
      "source": [
        "# Okt 형태소 분석기 객체를 생성합니다. 한국어 텍스트 분석을 위한 준비 단계입니다.\n",
        "okt = Okt()\n",
        "\n",
        "# 텍스트 데이터를 형태소 단위로 분석하여 특정 품사(명사, 형용사, 동사)만 추출하는 함수입니다.\n",
        "def tokenizer(text):\n",
        "    morph = okt.pos(text)\n",
        "    words = []\n",
        "    for word, tag in morph:\n",
        "        if tag in ['Noun', 'Adjective', 'Verb']:\n",
        "            if len(word) > 1:  # 한 글자보다 긴 단어만 선택\n",
        "                words.append(word)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz5ZZ6w62K5H"
      },
      "outputs": [],
      "source": [
        "text='나는 어제 영어 공부를 열심해 했다'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bXvGAFF2P0L"
      },
      "outputs": [],
      "source": [
        "okt = Okt()\n",
        "okt.pos(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuHiAbEI8608"
      },
      "outputs": [],
      "source": [
        "# 데이터프레임의 모든 데이터를 문자열 타입으로 변환합니다. 이는 형태소 분석을 위한 준비 단계입니다.\n",
        "df = df.astype('str')\n",
        "\n",
        "#'제목_본문' 컬럼에 tokenizer 함수를 적용하여, 형태소 분석을 수행하고 결과를 다시 해당 컬럼에 저장합니다.\n",
        "df['제목_본문'] = df['제목_본문'].apply(tokenizer)\n",
        "\n",
        "# 데이터프레임의 모든 열을 문자열 타입으로 변환합니다. 안전한 파일 저장을 위한 단계입니다.\n",
        "df = df.astype('str')\n",
        "\n",
        "# 데이터프레임을 '정제파일.csv'로 저장합니다. 분석 결과를 파일로 저장하는 단계입니다.\n",
        "df.to_csv('정제파일.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iuQrt5OvGdy"
      },
      "outputs": [],
      "source": [
        "df['제목_본문']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJYDK5CpcOjN"
      },
      "source": [
        "# 빈도분석 및 워드크라우드 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKjUv9_kbDhP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  # CountVectorizer 임포트\n",
        "df = pd.read_csv('/content/정제파일.csv', encoding='utf-8')\n",
        "#불용어를 처리합니다.\n",
        "stop_words=[\"코스피\", \"증시\"]\n",
        "\n",
        "cv = CountVectorizer(ngram_range=(1, 1), max_features=3000, stop_words=stop_words)###최대개수설정\n",
        "\n",
        "# CountVectorizer를 사용하여 '제목_본문' 컬럼의 텍스트 데이터를 문서-단어 행렬로 변환합니다.\n",
        "tdm = cv.fit_transform(df['제목_본문'])\n",
        "\n",
        "# 단어와 그 빈도수를 담은 데이터프레임을 생성합니다. 이는 단어 사용 빈도 분석을 위한 단계입니다.\n",
        "word_count_tf = pd.DataFrame({'단어': cv.get_feature_names_out(), '빈도': tdm.sum(axis=0).flat})\n",
        "\n",
        "# 단어 빈도수를 기준으로 내림차순으로 정렬합니다. 이는 가장 자주 사용된 단어를 확인하기 위한 단계입니다.\n",
        "word_count_tf = word_count_tf.sort_values('빈도', ascending=False)\n",
        "\n",
        "# 단어 빈도수를 'word_count.xlsx' 엑셀 파일로 저장합니다. 결과를 저장하는 단계입니다.\n",
        "word_count_tf.to_excel('word_count.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpbIQl6KbVPd"
      },
      "outputs": [],
      "source": [
        "# WordCloud 라이브러리를 임포트합니다: 텍스트 데이터의 빈도수를 시각화하기 위한 도구입니다.\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# 한글 폰트 경로를 지정합니다. WordCloud에서 한글을 정상적으로 표시하기 위한 준비 단계입니다.\n",
        "font_path = '/content/malgun.ttf'\n",
        "\n",
        "# WordCloud 객체를 생성합니다. 여기서는 배경색을 흰색으로, 최대 단어 수를 50개로 설정합니다.\n",
        "wc = WordCloud(background_color='white', max_words=50, font_path=font_path,width=400, height=400)\n",
        "\n",
        "# 단어 빈도 사전을 생성합니다. 이는 WordCloud 생성에 사용될 데이터입니다.\n",
        "count_dic = dict(zip(word_count_tf['단어'], word_count_tf['빈도']))\n",
        "\n",
        "# WordCloud를 이용해 단어 빈도를 시각화합니다. 이는 가장 많이 사용된 단어를 시각적으로 확인하기 위한 단계입니다.\n",
        "cloud = wc.fit_words(count_dic)\n",
        "cloud.to_file('word_cloud.png')\n",
        "cloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfHO78GvcJy2"
      },
      "source": [
        "# TF-IDF 및 워드크라우드 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL90Grw8b0JN"
      },
      "outputs": [],
      "source": [
        "# TfidfTransformer를 임포트합니다: TF-IDF 값을 계산하기 위한 도구입니다.\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# TfidfTransformer 객체를 생성합니다. 이는 문서-단어 행렬의 단어 빈도수를 TF-IDF 값으로 변환하기 위한 준비 단계입니다.\n",
        "trans = TfidfTransformer()\n",
        "\n",
        "# 문서-단어 행렬에 대해 TF-IDF 변환을 수행합니다. 이는 각 단어의 중요도를 수치화하는 단계입니다.\n",
        "dtm2 = trans.fit_transform(tdm)\n",
        "\n",
        "# 단어와 그 TF-IDF 값을 담은 데이터프레임을 생성합니다. 이는 단어의 중요도 분석을 위한 단계입니다.\n",
        "df2 = pd.DataFrame({'단어': cv.get_feature_names_out(), 'tf-idf': dtm2.sum(axis=0).flat})\n",
        "\n",
        "# TF-IDF 값을 기준으로 내림차순으로 정렬합니다. 이는 중요한 단어를 확인하기 위한 단계입니다.\n",
        "df2 = df2.sort_values('tf-idf', ascending=False)\n",
        "\n",
        "# 인덱스를 재설정합니다. 이는 데이터의 가독성을 높이기 위한 단계입니다.\n",
        "df2 = df2.reset_index(drop=True)\n",
        "df2.index = df2.index + 1\n",
        "\n",
        "# TF-IDF 값을 'tf_idf.csv' 파일로 저장합니다. 결과를 저장하는 단계입니다. 인코딩을 cp949로 설정하여 한글이 깨지지 않게 합니다.\n",
        "df2.to_csv('tf_idf.csv', encoding='cp949')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7Oks-QEb7X-"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud  # WordCloud 임포트\n",
        "\n",
        "# 한글 폰트 경로 지정\n",
        "font_path = '/content/malgun.ttf'\n",
        "\n",
        "# WordCloud 객체를 다시 생성합니다. 이번에는 TF-IDF 값을 기반으로 가장 중요한 단어를 시각화합니다.\n",
        "wc = WordCloud(background_color='black', max_words=30, font_path=font_path,width=400, height=400)\n",
        "\n",
        "# TF-IDF 값을 기반으로 단어 빈도 사전을 생성합니다.\n",
        "count_dic = dict(zip(df2['단어'], df2['tf-idf']))\n",
        "\n",
        "# WordCloud를 이용해 TF-IDF 값을 시각화합니다. 이는 중요한 단어를 시각적으로 확인하기 위한 단계입니다.\n",
        "cloud = wc.fit_words(count_dic)\n",
        "cloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNAJ4kwBcHAQ"
      },
      "source": [
        "# LDA분석"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyldavis"
      ],
      "metadata": {
        "id": "5KEax10oqfCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpROVhl9cQsj"
      },
      "outputs": [],
      "source": [
        "# LDA 분석을 위한 라이브러리 임포트\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import pyLDAvis\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgx5d43hcqQS"
      },
      "outputs": [],
      "source": [
        "# CSV 파일에서 데이터 불러오기\n",
        "data = pd.read_csv('/content/정제파일.csv', encoding='utf-8')\n",
        "data = data.astype('str')\n",
        "data=data['제목_본문']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssnkil3rv5qL"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJkOV3vCcvvR"
      },
      "outputs": [],
      "source": [
        "# 'data' 리스트에 저장된 텍스트 데이터(예: '제목_본문')를 공백 기준으로 분할하여 토큰 리스트를 생성합니다.\n",
        "token = [keywords.split() for keywords in data]\n",
        "# 토큰 리스트를 기반으로 Gensim의 Dictionary 객체를 생성합니다. 이 객체는 각 토큰에 고유한 ID를 매핑합니다.\n",
        "# 결과적으로, 텍스트 데이터의 모든 유니크한 단어가 ID와 함께 딕셔너리로 관리됩니다.\n",
        "id2word = corpora.Dictionary(token)\n",
        "\n",
        "# 딕셔너리에서 빈도수 기반으로 단어를 필터링합니다. no_below와 no_above 파라미터를 사용하여,\n",
        "# 너무 드물게 나타나는 단어(no_below 이하)와 너무 자주 나타나는 단어(no_above 이상)를 제거합니다.\n",
        "# no_below=3는 각 단어가 최소 3개의 문서에 나타나야 함을 의미합니다.\n",
        "# no_above=0.1는 전체 문서의 10% 미만에만 나타나는 단어를 유지함을 의미합니다.\n",
        "id2word.filter_extremes(no_below=5, no_above=0.2)\n",
        "\n",
        "# 최종적으로, 각 문서를 (단어 ID, 단어 빈도) 튜플의 리스트로 변환하는 문서-단어 빈도 매트릭스(corpus)를 생성합니다.\n",
        "# 이 과정에서 각 문서 내에 있는 단어들이 얼마나 자주 나타나는지를 기록합니다.\n",
        "# 이 corpus는 텍스트 데이터를 기반으로 한 다양한 모델링 작업(예: LDA 주제 모델링)에 사용될 수 있습니다.\n",
        "corpus = [id2word.doc2bow(text) for text in token]\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N08gYAj4cybD"
      },
      "outputs": [],
      "source": [
        "# numpy 라이브러리를 np라는 이름으로 가져옵니다. numpy는 다양한 수학적 연산을 위해 사용됩니다.\n",
        "import numpy as np\n",
        "\n",
        "perplexity_values = []\n",
        "\n",
        "# 토픽의 수를 3에서 9까지 변화시키면서 LDA 모델의 퍼플렉시티를 계산합니다.\n",
        "# 토픽 수가 변할 때마다 모델의 성능을 퍼플렉시티를 통해 평가합니다.\n",
        "for i in range(3, 10):\n",
        "    # LDA 모델을 생성합니다. 'num_topics=i'는 토픽의 수를 i로 설정합니다.\n",
        "    # 'id2word'는 단어의 인덱스 매핑을 나타내는 사전입니다.\n",
        "    # 'random_state=100'는 모델 결과의 재현 가능성을 위해 난수 생성기의 시드를 설정합니다.\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=id2word, random_state=100)\n",
        "\n",
        "    # 모델의 로그 퍼플렉시티를 계산하고, 이를 자연로그의 밑인 e를 기반으로 하는 지수 함수로 변환하여\n",
        "    # 퍼플렉시티를 계산합니다. 퍼플렉시티 값이 낮을수록 모델이 데이터를 더 잘 설명하고 있다는 의미입니다.\n",
        "    perplexity = np.exp(ldamodel.log_perplexity(corpus))\n",
        "\n",
        "    # 계산된 퍼플렉시티를 리스트에 추가합니다.\n",
        "    perplexity_values.append(perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4gVBW6fczn7"
      },
      "outputs": [],
      "source": [
        "# Perplexity 그래프 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = range(3, 10)\n",
        "plt.plot(x, perplexity_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"perplexity score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZWtEBMqc2fk"
      },
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel  # CoherenceModel 임포트\n",
        "\n",
        "coherence_values = []\n",
        "# 다양한 토픽 수에 대해 Coherence 계산\n",
        "for i in range(3, 10):\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=id2word,random_state=100)\n",
        "    coherence_model_lda = CoherenceModel(model=ldamodel, texts=token, dictionary=id2word, topn=10)\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    coherence_values.append(coherence_lda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLSUTNf-c3zl"
      },
      "outputs": [],
      "source": [
        "x = range(3, 10)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"number of topics\")\n",
        "plt.ylabel(\"coherence score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwsfxhGYKGGi"
      },
      "outputs": [],
      "source": [
        "k=4 ###적정토픽수 입력 coherence를 최대로 하고 perpelxity를 최소로 하는 적절합 토픽수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQNv_E8qc5Al"
      },
      "outputs": [],
      "source": [
        "#적정토픽에 해당 되는 단어를 구하기!\n",
        "from gensim import corpora, models\n",
        "# 적절한 토픽 수를 설정하여 LDA 모델 훈련\n",
        "model = LdaModel(corpus=corpus, num_topics=k, id2word=id2word, passes=15,random_state=100)  # num_topic에 적절 토픽 수 설정\n",
        "topics = model.print_topics(num_words=15) ####토픽당 단어수설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENc9IoaSc6Qx"
      },
      "outputs": [],
      "source": [
        "topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuAbGyx4c7Ys"
      },
      "outputs": [],
      "source": [
        "# 도출된 토픽을 CSV 파일로 저장\n",
        "TOPIC={'topics':topics}\n",
        "TOPIC=pd.DataFrame(TOPIC)\n",
        "TOPIC.to_csv('lda_result.csv',encoding='cp949')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyldavis"
      ],
      "metadata": {
        "id": "9AJmHbygHrs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook() # Jupyter Notebook에서 pyLDAvis 시각화를 사용할 수 있도록 설정\n",
        "# pyLDAvis를 이용해 LDA 모델 시각화\n",
        "vis = pyLDAvis.gensim.prepare(model, corpus, id2word)\n",
        "vis  # 시각화 객체 출력"
      ],
      "metadata": {
        "id": "Vmn-jOAHHEqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}